{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b685c1-fe44-4f01-b9e3-5808cf10752f",
   "metadata": {},
   "source": [
    "# Chat With Your Data\n",
    "\n",
    "## Preserve conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed94b7-d5da-4c5b-a812-25ecbb1601a6",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603889ff-e355-4c9e-929a-45e8150aa0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ae117-0aa2-4c09-b5bd-7ce1e106ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd511c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca531938",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1839e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c527ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fcee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchainhub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef23fe7-5c16-4f30-97e4-b679c75e716d",
   "metadata": {},
   "source": [
    "## Load OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77f69144-98d1-46a1-94be-8eea00065278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "OPENAI_API_KEY=os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959501e7-3a7c-456e-93f1-0561979e81c4",
   "metadata": {},
   "source": [
    "## Prompt model with no knowledge of the Voynich manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c71c0d74-1911-4082-8bc2-c930607fb800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "#initialize the LLM we'll use - OpenAI GPT 3.5 Turbo\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67a63a96-ddfb-4beb-a699-d2c035ae67cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The Voynich manuscript is a mysterious and undeciphered text believed to have been written in the early 15th century. It contains illustrations of plants, astronomical diagrams, and what appear to be medicinal recipes. Some scholars have suggested that the manuscript may contain valuable insights into medieval medicine and herbal remedies. However, due to the inability to translate the text, the exact medicinal insights from the Voynich manuscript remain unknown. Some theories suggest that it may contain information on herbal remedies, alchemy, or other medical practices of the time period. Further research and analysis are needed to fully understand the medicinal insights that may be contained within the Voynich manuscript.', response_metadata={'token_usage': {'completion_tokens': 133, 'prompt_tokens': 19, 'total_tokens': 152}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prompt the model with no additional knowledge of the Voynich manuscript beyond pretraining \n",
    "llm.invoke(\"What are the medicinal insights from the Voynich manuscript?\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "135b5589-664b-4ce7-aa2f-d593e9758c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Aetherfloris Ventus is a Latin term that translates to \"Ether Flower Wind\" in English. It is a poetic and mystical phrase that could potentially refer to a concept or element of nature that combines the qualities of ether, flowers, and wind. It may be used in literature, art, or philosophy to evoke a sense of beauty, fragility, and movement.', response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 16, 'total_tokens': 91}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is Aetherfloris Ventus?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafecdae-eaf9-471f-b69d-9b1c3ac0b9d2",
   "metadata": {},
   "source": [
    "## Load vector database from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0788aef1-0fe8-4205-ba3d-741bb5cf392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "db = FAISS.load_local(\"../faiss_index\", \n",
    "                      OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\"), \n",
    "                      allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b7e3f2-eaba-4fa1-aced-d79431959cd9",
   "metadata": {},
   "source": [
    "## Configure retriever\n",
    "### Use the similarity search capabilities of a vector store to facilitate retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ec80ca7-918a-4898-b64d-4cee56d8141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4dfe35-f043-45c1-a244-0e4505807af4",
   "metadata": {},
   "source": [
    "## Implement a chain\n",
    "### Chain together multiple calls in a logical sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3390bcda-1b75-4eaf-96b6-73c583c08760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbe8886a-c809-428c-b2ff-b8d4c1b933f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89b37521-0732-49cd-996c-5a955d64df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#combine multiple steps in a single chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser() #convert the chat message to a string\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd0fdc-8a89-42f5-9588-c728c99aaafa",
   "metadata": {},
   "source": [
    "## Send LLM's response to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33d8b346-435b-4d50-a9b3-729e90bde815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Voynich manuscript contains detailed anatomical diagrams of mythical beings with annotations explaining organ functions, possibly used for medicinal or alchemical purposes. It also features illustrations of herbs with unique properties, indicating potential medicinal uses, and herbal brews and potions with specific effects on health and vitality. The manuscript offers insights into ancient medical and alchemical practices, showcasing a blend of fantastical biology and practical medicinal knowledge."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What are the medicinal insights from the Voynich manuscript?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6667483b-9d8d-46f7-bbf9-7417249da5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aetherfloris Ventus is an ethereal plant with petals lighter than air, appearing to float freely, nurtured by the winds and clouds. Its stem is nearly invisible, leading the delicate petals in a captivating ballet. The plant's essence is believed to induce lightness in the body and mind, challenging our understanding of the natural world."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What is Aetherfloris Ventus?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93d4f8e0-14cb-40d0-b710-542d27c31bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most important part of the Voynich Manuscript is the detailed anatomical diagrams of mythical beings, possibly used for medicinal or alchemical purposes, with annotations explaining the function of each organ and system. These diagrams offer a glimpse into ancient medical knowledge intertwined with fantasy, possibly serving as a practical guide for medicinal and alchemical uses. The anatomical details and annotations within the manuscript provide insights into the functions of various organs and their potential importance in ancient practices."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What's the most important part of the Voynich manuscript?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc0fd64-aa2e-46fe-a2c6-a0b8ca109d76",
   "metadata": {},
   "source": [
    "## Preserve Conversation History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "732ea316-19cb-435f-b822-fb0647925980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system_prompt = \"\"\"Given the chat history and a recent user question \\\n",
    "generate a new standalone question \\\n",
    "that can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed or otherwise return it as is.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever_with_history = create_history_aware_retriever(\n",
    "    llm, retriever, prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6593d472-89cc-4b7b-b61a-dad74e864d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given the chat history and a recent user question generate a new standalone question that can be understood without the chat history. Do NOT answer the question, just reformulate it if needed or otherwise return it as is.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e56844fc-4d86-4cfb-85c9-1ca55794646f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chat_history', 'input']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77d1cc27-1dd6-4210-8eab-c44e1802afd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aecbdb5a-a390-4e4e-927a-501b6e4b232e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given the chat history and a recent user question generate a new standalone question that can be understood without the chat history. Do NOT answer the question, just reformulate it if needed or otherwise return it as is.')),\n",
       " MessagesPlaceholder(variable_name='chat_history'),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "682e7231-0714-4533-8410-9a11306b7506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(...), RunnableLambda(...)\n",
       "| VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x10c031790>, search_kwargs={'k': 6}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given the chat history and a recent user question generate a new standalone question that can be understood without the chat history. Do NOT answer the question, just reformulate it if needed or otherwise return it as is.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10c0776d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10bd5cdf0>, model_name='gpt-3.5-turbo-0125', openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x10c031790>, search_kwargs={'k': 6})), config={'run_name': 'chat_retriever_chain'})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_with_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
